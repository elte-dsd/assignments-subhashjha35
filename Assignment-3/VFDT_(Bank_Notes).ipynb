{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VFDT (Bank Notes).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSkxw_gdhNzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code Source : https://github.com/1634113866/incremental_decision_tree-CART-Random_Forest_python/blob/master/vfdt.py\n",
        "# Research Paper used: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf\n",
        "# DataSet used: https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCmL1_WCchLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.utils import check_array, check_X_y\n",
        "from sklearn import tree\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwPB_XC8ba04",
        "colab_type": "code",
        "outputId": "a957359c-4bb6-4e4d-dd44-a25240fd299c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!curl -o banknote.txt https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 46400  100 46400    0     0   227k      0 --:--:-- --:--:-- --:--:--  226k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xemczWvcvfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Very Fast Decision Tree i.e. Hoeffding Tree, described in\n",
        "# \"Mining High-Speed Data Streams\" (Domingos & Hulten, 2000)\n",
        "#\n",
        "# this program contains 2 classes: Vfdt, VfdtNode\n",
        "# changed to CART: gini index\n",
        "# VFDT node class\n",
        "\n",
        "class VfdtNode:\n",
        "    def __init__(self, possible_split_features):\n",
        "        \"\"\"\n",
        "        nijk: statistics of feature i, value j, class\n",
        "        possible_split_features: features list\n",
        "        \"\"\"\n",
        "        self.parent = None\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.split_feature = None\n",
        "        self.split_value = None  # both continuous and discrete value\n",
        "        self.new_examples_seen = 0\n",
        "        self.total_examples_seen = 0\n",
        "        self.class_frequency = {}\n",
        "        self.nijk = {f: {} for f in possible_split_features}\n",
        "        self.possible_split_features = possible_split_features\n",
        "\n",
        "    def add_children(self, split_feature, split_value, left, right):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_value = split_value\n",
        "        self.left_child = left\n",
        "        self.right_child = right\n",
        "        left.parent = self\n",
        "        right.parent = self\n",
        "\n",
        "        self.nijk.clear()  # reset stats\n",
        "        if isinstance(split_value, list):\n",
        "            left_value = split_value[0]\n",
        "            right_value = split_value[1]\n",
        "            # discrete split value list's length = 1, stop splitting\n",
        "            if len(left_value) <= 1:\n",
        "                new_features = [None if f == split_feature else f\n",
        "                                for f in left.possible_split_features]\n",
        "                left.possible_split_features = new_features\n",
        "            if len(right_value) <= 1:\n",
        "                new_features = [None if f == split_feature else f\n",
        "                                for f in right.possible_split_features]\n",
        "                right.possible_split_features = new_features\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.left_child is None and self.right_child is None\n",
        "\n",
        "    # recursively trace down the tree\n",
        "    # to distribute data examples to corresponding leaves\n",
        "    def sort_example(self, x):\n",
        "        if self.is_leaf():\n",
        "            return self\n",
        "        else:\n",
        "            index = self.possible_split_features.index(self.split_feature)\n",
        "            value = x[index]\n",
        "            split_value = self.split_value\n",
        "\n",
        "            if isinstance(split_value, list):  # discrete value\n",
        "                if value in split_value[0]:\n",
        "                    return self.left_child.sort_example(x)\n",
        "                else:\n",
        "                    return self.right_child.sort_example(x)\n",
        "            else:  # continuous value\n",
        "                if value <= split_value:\n",
        "                    return self.left_child.sort_example(x)\n",
        "                else:\n",
        "                    return self.right_child.sort_example(x)\n",
        "\n",
        "    # the most frequent class\n",
        "    def most_frequent(self):\n",
        "        try:\n",
        "            prediction = max(self.class_frequency,\n",
        "                             key=self.class_frequency.get)\n",
        "        except ValueError:\n",
        "            # if self.class_frequency dict is empty, go back to parent\n",
        "            class_frequency = self.parent.class_frequency\n",
        "            prediction = max(class_frequency, key=class_frequency.get)\n",
        "        return prediction\n",
        "\n",
        "    # update leaf stats in order to calculate gini\n",
        "    def update_stats(self, x, y):\n",
        "        feats = self.possible_split_features\n",
        "        nijk = self.nijk\n",
        "        iterator = [f for f in feats if f is not None]\n",
        "        for i in iterator:\n",
        "            value = x[feats.index(i)]\n",
        "            if value not in nijk[i]:\n",
        "                nijk[i][value] = {y: 1}\n",
        "            else:\n",
        "                try:\n",
        "                    nijk[i][value][y] += 1\n",
        "                except KeyError:\n",
        "                    nijk[i][value][y] = 1\n",
        "\n",
        "        self.total_examples_seen += 1\n",
        "        self.new_examples_seen += 1\n",
        "        class_frequency = self.class_frequency\n",
        "        try:\n",
        "            class_frequency[y] += 1\n",
        "        except KeyError:\n",
        "            class_frequency[y] = 1\n",
        "\n",
        "    def check_not_splitting(self):\n",
        "        # compute gini index for not splitting\n",
        "        X0 = 1\n",
        "        class_frequency = self.class_frequency\n",
        "        n = sum(class_frequency.values())\n",
        "        for j, k in class_frequency.items():\n",
        "            X0 -= (k/n)**2\n",
        "        return X0\n",
        "\n",
        "    # use Hoeffding tree model to test node split, return the split feature\n",
        "    def attempt_split(self, delta, nmin, tau):\n",
        "        if self.new_examples_seen < nmin:\n",
        "            return None\n",
        "        class_frequency = self.class_frequency\n",
        "        if len(class_frequency) == 1:\n",
        "            return None\n",
        "\n",
        "        self.new_examples_seen = 0  # reset\n",
        "        nijk = self.nijk\n",
        "        min = 1\n",
        "        second_min = 1\n",
        "        Xa = ''\n",
        "        split_value = None\n",
        "        for feature in self.possible_split_features:\n",
        "            if feature is not None:\n",
        "                njk = nijk[feature]\n",
        "                gini, value = self.gini(njk, class_frequency)\n",
        "                if gini < min:\n",
        "                    min = gini\n",
        "                    Xa = feature\n",
        "                    split_value = value\n",
        "                elif min < gini < second_min:\n",
        "                    second_min = gini\n",
        "\n",
        "        epsilon = self.hoeffding_bound(delta)\n",
        "        g_X0 = self.check_not_splitting()\n",
        "        if min < g_X0:\n",
        "            # print(second_min - min, epsilon)\n",
        "            if second_min - min > epsilon:\n",
        "                # print('1 node split')\n",
        "                return [Xa, split_value]\n",
        "            elif tau != 0 and second_min - min < epsilon < tau:\n",
        "                # print('2 node split')\n",
        "                return [Xa, split_value]\n",
        "            else:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def hoeffding_bound(self, delta):\n",
        "        n = self.total_examples_seen\n",
        "        R = np.log(len(self.class_frequency))\n",
        "        return np.sqrt(R * R * np.log(1/delta) / (2 * n))\n",
        "\n",
        "    def gini(self, njk, class_frequency):\n",
        "        # gini(D) = 1 - Sum(pi^2)\n",
        "        # gini(D, F=f) = |D1|/|D|*gini(D1) + |D2|/|D|*gini(D2)\n",
        "\n",
        "        D = self.total_examples_seen\n",
        "        m1 = 1  # minimum gini\n",
        "        # m2 = 1  # second minimum gini\n",
        "        Xa_value = None\n",
        "        feature_values = list(njk.keys())  # list() is essential\n",
        "        if not isinstance(feature_values[0], str):  # numeric  feature values\n",
        "            sort = np.array(sorted(feature_values))\n",
        "            # vectorized computation, like in R\n",
        "            split = (sort[0:-1] + sort[1:])/2\n",
        "\n",
        "            D1_class_frequency = {j: 0 for j in class_frequency.keys()}\n",
        "            for index in range(len(split)):\n",
        "                nk = njk[sort[index]]\n",
        "                for j in nk:\n",
        "                    D1_class_frequency[j] += nk[j]\n",
        "                D1 = sum(D1_class_frequency.values())\n",
        "                D2 = D - D1\n",
        "                g_d1 = 1\n",
        "                g_d2 = 1\n",
        "\n",
        "                D2_class_frequency = {}\n",
        "                for key, value in class_frequency.items():\n",
        "                    if key in D1_class_frequency:\n",
        "                        D2_class_frequency[key] = value - \\\n",
        "                            D1_class_frequency[key]\n",
        "                    else:\n",
        "                        D2_class_frequency[key] = value\n",
        "\n",
        "                for key, v in D1_class_frequency.items():\n",
        "                    g_d1 -= (v/D1)**2\n",
        "                for key, v in D2_class_frequency.items():\n",
        "                    g_d2 -= (v/D2)**2\n",
        "                g = g_d1*D1/D + g_d2*D2/D\n",
        "                if g < m1:\n",
        "                    m1 = g\n",
        "                    Xa_value = split[index]\n",
        "                # elif m1 < g < m2:\n",
        "                    # m2 = g\n",
        "            return [m1, Xa_value]\n",
        "\n",
        "        else:  # discrete feature_values\n",
        "            length = len(njk)\n",
        "            if length > 10:  # too many discrete feature values, estimate\n",
        "                for j, k in njk.items():\n",
        "                    D1 = sum(k.values())\n",
        "                    D2 = D - D1\n",
        "                    g_d1 = 1\n",
        "                    g_d2 = 1\n",
        "\n",
        "                    D2_class_frequency = {}\n",
        "                    for key, value in class_frequency.items():\n",
        "                        if key in k:\n",
        "                            D2_class_frequency[key] = value - k[key]\n",
        "                        else:\n",
        "                            D2_class_frequency[key] = value\n",
        "                    for key, v in k.items():\n",
        "                        g_d1 -= (v/D1)**2\n",
        "\n",
        "                    if D2 != 0:\n",
        "                        for key, v in D2_class_frequency.items():\n",
        "                            g_d2 -= (v/D2)**2\n",
        "                    g = g_d1*D1/D + g_d2*D2/D\n",
        "                    if g < m1:\n",
        "                        m1 = g\n",
        "                        Xa_value = j\n",
        "                    # elif m1 < g < m2:\n",
        "                        # m2 = g\n",
        "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
        "\n",
        "            else:  # fewer discrete feature values, get combinations\n",
        "                comb = self.select_combinations(feature_values)\n",
        "                for i in comb:\n",
        "                    left = list(i)\n",
        "                    D1_class_frequency = {\n",
        "                        key: 0 for key in class_frequency.keys()}\n",
        "                    D2_class_frequency = {\n",
        "                        key: 0 for key in class_frequency.keys()}\n",
        "                    for j, k in njk.items():\n",
        "                        for key, value in class_frequency.items():\n",
        "                            if j in left:\n",
        "                                if key in k:\n",
        "                                    D1_class_frequency[key] += k[key]\n",
        "                            else:\n",
        "                                if key in k:\n",
        "                                    D2_class_frequency[key] += k[key]\n",
        "                    g_d1 = 1\n",
        "                    g_d2 = 1\n",
        "                    D1 = sum(D1_class_frequency.values())\n",
        "                    D2 = D - D1\n",
        "                    for key, v in D1_class_frequency.items():\n",
        "                        g_d1 -= (v/D1)**2\n",
        "                    for key, v in D2_class_frequency.items():\n",
        "                        g_d2 -= (v/D2)**2\n",
        "                    g = g_d1*D1/D + g_d2*D2/D\n",
        "                    if g < m1:\n",
        "                        m1 = g\n",
        "                        Xa_value = left\n",
        "                    # elif m1 < g < m2:\n",
        "                        # m2 = g\n",
        "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
        "            return [m1, [Xa_value, right]]\n",
        "\n",
        "    # divide values into two groups, return the combination of left groups\n",
        "    def select_combinations(self, feature_values):\n",
        "        combination = []\n",
        "        e = len(feature_values)\n",
        "        if e % 2 == 0:\n",
        "            end = int(e/2)\n",
        "            for i in range(1, end+1):\n",
        "                if i == end:\n",
        "                    cmb = list(combinations(feature_values, i))\n",
        "                    enough = int(len(cmb)/2)\n",
        "                    combination.extend(cmb[:enough])\n",
        "                else:\n",
        "                    combination.extend(combinations(feature_values, i))\n",
        "        else:\n",
        "            end = int((e-1)/2)\n",
        "            for i in range(1, end+1):\n",
        "                combination.extend(combinations(feature_values, i))\n",
        "\n",
        "        return combination"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGI62HbIZRVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# very fast decision tree class, i.e. hoeffding tree\n",
        "class Vfdt:\n",
        "    def __init__(self, features, delta=0.01, nmin=100, tau=0.1):\n",
        "        \"\"\"\n",
        "        :features: list of data features\n",
        "        :delta: used to compute hoeffding bound, error rate\n",
        "        :nmin: to limit the G computations\n",
        "        :tau: to deal with ties\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.delta = delta\n",
        "        self.nmin = nmin\n",
        "        self.tau = tau\n",
        "        self.root = VfdtNode(features)\n",
        "        self.n_examples_processed = 0\n",
        "        # print(self.features, self.delta, self.tau, self.n_examples_processed)\n",
        "        # self.print_tree()\n",
        "        # print(\"--- / __init__ ---\")\n",
        "\n",
        "    # update the tree by adding one or many training example(s)\n",
        "    def update(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        for x, _y in zip(X, y):\n",
        "            self.__update(x, _y)\n",
        "\n",
        "    # update the tree by adding one training example\n",
        "    def __update(self, x, _y):\n",
        "        self.n_examples_processed += 1\n",
        "        node = self.root.sort_example(x)\n",
        "        node.update_stats(x, _y)\n",
        "\n",
        "        result = node.attempt_split(self.delta, self.nmin, self.tau)\n",
        "        if result is not None:\n",
        "            feature = result[0]\n",
        "            value = result[1]\n",
        "            self.node_split(node, feature, value)\n",
        "\n",
        "    # split node, produce children\n",
        "    def node_split(self, node, split_feature, split_value):\n",
        "        features = node.possible_split_features\n",
        "        # print('node_split')\n",
        "        left = VfdtNode(features)\n",
        "        right = VfdtNode(features)\n",
        "        node.add_children(split_feature, split_value, left, right)\n",
        "\n",
        "    # predict test example's classification\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        return [self.__predict(x) for x in X]\n",
        "        # if isinstance(X, (np.ndarray, list)):\n",
        "        #     return [self.__predict(x) for x in X]\n",
        "        # else:\n",
        "        #     leaf = self.__predict(X)\n",
        "\n",
        "    def __predict(self, x):\n",
        "        leaf = self.root.sort_example(x)\n",
        "        return leaf.most_frequent()\n",
        "\n",
        "    def print_tree(self, node=None):\n",
        "        if node is None:\n",
        "            self.print_tree(self.root)\n",
        "        elif node.is_leaf():\n",
        "            print('Leaf')\n",
        "        else:\n",
        "            print(node.split_feature)\n",
        "            self.print_tree(node.left_child)\n",
        "            self.print_tree(node.right_child)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZqzqqz6dG75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_run():\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = pd.read_csv('banknote.txt', skiprows=0, header=0)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # shuffle data rows\n",
        "    title = list(df.columns.values)\n",
        "    print(\"title\", title)\n",
        "    features = title[:-1]\n",
        "    rows = df.shape[0]\n",
        "\n",
        "    # print(\"Features:\", features)\n",
        "\n",
        "    n_training = 1000\n",
        "    array = df.head(n_training).values\n",
        "\n",
        "    set1 = array[:200, :]\n",
        "    set2 = array[200:400, :]\n",
        "    set3 = array[400:600, :]\n",
        "    set4 = array[800:1000, :]\n",
        "\n",
        "    # to simulate continuous training, modify the tree for each training set\n",
        "    examples = [set1, set2, set3, set4]\n",
        "\n",
        "    # test set is different from training set\n",
        "    n_test = 371\n",
        "    test_set = df.tail(n_test).values\n",
        "    x_test = test_set[:, :-1]\n",
        "    y_test = test_set[:, -1]\n",
        "\n",
        "    # Heoffding bound (epsilon) parameter delta: with 1 - delta probability\n",
        "    # the true mean is at least r_bar - epsilon\n",
        "    # Efdt parameter nmin: test split if new sample size > nmin\n",
        "    # feature_values: unique values in every feature\n",
        "    # tie breaking: when difference is so small,\n",
        "    # split when diff_g < epsilon < tau\n",
        "    htree = Vfdt(features, delta=0.01, nmin=10, tau=0.5)\n",
        "    print('Total data size: ', rows)\n",
        "    print('Training size: ', n_training)\n",
        "    print('Test set size: ', n_test)\n",
        "    n = 0\n",
        "\n",
        "    print(\"---- VFDT ----\")\n",
        "    for training_set in examples:\n",
        "        n += len(training_set)\n",
        "        x_train = training_set[:, :-1]\n",
        "        y_train = training_set[:, -1]\n",
        "        htree.update(x_train, y_train)\n",
        "        y_pred = htree.predict(x_test)\n",
        "\n",
        "        print('Training set:', n, end=', ')\n",
        "        print('ACCURACY: %.4f' % accuracy_score(y_test, y_pred))\n",
        "\n",
        "    # tree.print_tree(tree.root)\n",
        "    print(\"--- Running time: %.6f seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    metrics = list(\n",
        "        precision_recall_fscore_support(\n",
        "            y_test, y_pred, average='weighted',\n",
        "            labels=np.unique(y_pred)))\n",
        "    metrics = pd.DataFrame({\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': metrics[0],\n",
        "        'recall': metrics[1],\n",
        "        'f1': metrics[2]}, index=[0])\n",
        "    print(metrics)\n",
        "    print(\"----SKLEARN DECISION TREE----\")\n",
        "    X_train1 = array[:1000, :-1]\n",
        "    y_train1 = array[:1000, -1]\n",
        "    # print(y_train1)\n",
        "    start_time1 = time.time()\n",
        "    clf = tree.DecisionTreeClassifier(max_depth= 4, min_samples_leaf= 2, random_state=42)\n",
        "    clf = clf.fit(X_train1, y_train1)\n",
        "    # print(test_set)\n",
        "    y_pred1 = clf.predict(test_set[:, :-1])\n",
        "    y_test1 = test_set[:, -1]\n",
        "    # print(y_test)\n",
        "    metrics1 = pd.DataFrame({\n",
        "        'accuracy': accuracy_score(y_test1, y_pred1)}, index=[1])\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Running time: %.6f seconds\" ,(time.time() - start_time1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sadr5CEmdNBu",
        "colab_type": "code",
        "outputId": "d01a08dc-b8dd-47f7-962b-a66cc3821503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#if __name__ == \"__main__\":\n",
        "test_run()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title ['3.6216', '8.6661', '-2.8073', '-0.44699', '0']\n",
            "Total data size:  1371\n",
            "Training size:  1000\n",
            "Test set size:  371\n",
            "---- VFDT ----\n",
            "Training set: 200, ACCURACY: 0.9299\n",
            "Training set: 400, ACCURACY: 0.9677\n",
            "Training set: 600, ACCURACY: 0.9677\n",
            "Training set: 800, ACCURACY: 0.9677\n",
            "--- Running time: 0.040067 seconds ---\n",
            "   accuracy  precision    recall        f1\n",
            "0  0.967655   0.967749  0.967655  0.967674\n",
            "----SKLEARN DECISION TREE----\n",
            "Accuracy: 0.967654986522911\n",
            "Running time: %.6f seconds 0.004366159439086914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcbMJ3QApixJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}